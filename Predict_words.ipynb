{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRXKzy7pCuQdEizQmvzmcZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAdewunmi/Next-Word-Prediction-Project/blob/main/Predict_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Words!"
      ],
      "metadata": {
        "id": "fJR9MBr5IIYr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "478d04b6"
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import pkg_resources\n",
        "import pickle\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "import numpy\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "WWfT8jo31nfn"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add utility functions for document handling and sample data directory detection\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "import string\n",
        "\n",
        "def _detect_sample_dir() -> Path:\n",
        "    \"\"\"\n",
        "    Return a usable /sample_data/ directory for the current environment.\n",
        "\n",
        "    Priority:\n",
        "      1) /sample_data (if it exists)\n",
        "      2) /content/sample_data (Colab default)\n",
        "      3) Create /sample_data if neither exists (for local/Jupyter)\n",
        "    \"\"\"\n",
        "    candidates = [Path(\"/sample_data\"), Path(\"/content/sample_data\")]\n",
        "    for d in candidates:\n",
        "        if d.exists():\n",
        "            return d\n",
        "    # Create a local /sample_data if nothing exists\n",
        "    d = Path(\"/sample_data\")\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "    return d\n",
        "\n",
        "SAMPLE_DIR = _detect_sample_dir()\n",
        "\n",
        "def load_doc(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Read the entire contents of a text file.\n",
        "\n",
        "    Args:\n",
        "        filename: Path to the text file (absolute or relative).\n",
        "\n",
        "    Returns:\n",
        "        File contents as a single string.\n",
        "    \"\"\"\n",
        "    path = Path(filename)\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def clean_doc(doc: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Convert raw document text into cleaned, lowercased, alphabetic tokens.\n",
        "\n",
        "    Steps:\n",
        "      1) Replace double hyphens with a space.\n",
        "      2) Split on whitespace.\n",
        "      3) Remove ASCII punctuation from each token.\n",
        "      4) Keep only purely alphabetic tokens (isalpha()).\n",
        "      5) Lowercase all tokens.\n",
        "\n",
        "    Args:\n",
        "        doc: Raw document text.\n",
        "\n",
        "    Returns:\n",
        "        List of cleaned tokens.\n",
        "    \"\"\"\n",
        "    doc = doc.replace(\"--\", \" \")\n",
        "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    tokens = [w.translate(table) for w in doc.split()]\n",
        "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "def save_doc(lines: List[str], filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Save a list of strings to disk, one per line.\n",
        "\n",
        "    Args:\n",
        "        lines: Strings to write (e.g., tokens).\n",
        "        filename: Output file path to write.\n",
        "    \"\"\"\n",
        "    path = Path(filename)\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n"
      ],
      "metadata": {
        "id": "f17Zepp5XICk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add script to tokenize text file from sample_data and save tokens\n",
        "\n",
        "# Point to files in /sample_data (or /content/sample_data if that's what exists)\n",
        "INPUT_FILE = (SAMPLE_DIR / \"republic.txt\").as_posix()\n",
        "OUTPUT_FILE = (SAMPLE_DIR / \"republic-tokenised.txt\").as_posix()\n",
        "\n",
        "# Execute the pipeline\n",
        "text = load_doc(INPUT_FILE)\n",
        "tokens = clean_doc(text)\n",
        "save_doc(tokens, OUTPUT_FILE)\n",
        "\n",
        "print(f\"Read from: {INPUT_FILE}\")\n",
        "print(f\"Wrote  to: {OUTPUT_FILE}\")\n",
        "print(f\"Sample tokens: {tokens[:25]}\")\n",
        "print(f\"Total tokens: {len(tokens):,}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjlLnFTB1ptw",
        "outputId": "16f88799-3518-4dd1-d99c-41ac6c918e0b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read from: /content/sample_data/republic.txt\n",
            "Wrote  to: /content/sample_data/republic-tokenised.txt\n",
            "Sample tokens: ['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'republic', 'by', 'plato', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other']\n",
            "Total tokens: 209,695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add sanity checks for document processing pipeline\n",
        "\n",
        "# Minimal assertions to catch common regressions\n",
        "assert isinstance(text, str) and len(text) > 0, \"Input text is empty.\"\n",
        "assert all(isinstance(t, str) for t in tokens), \"Tokens must be strings.\"\n",
        "assert all(t.isalpha() for t in tokens), \"Non-alphabetic tokens slipped through.\"\n",
        "assert Path(OUTPUT_FILE).exists(), \"Output file was not written.\"\n",
        "\n",
        "# Round-trip check on saver/loader behavior\n",
        "tmp_out = (SAMPLE_DIR / \"_tmp_tokens.txt\").as_posix()\n",
        "save_doc([\"A\", \"b\", \"c\"], tmp_out)\n",
        "reloaded = load_doc(tmp_out).splitlines()\n",
        "assert reloaded == [\"A\", \"b\", \"c\"], \"save_doc/load_doc round-trip failed.\"\n",
        "Path(tmp_out).unlink(missing_ok=True)\n",
        "\n",
        "print(\"Sanity checks passed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpoaejgg5a5C",
        "outputId": "329b9345-ad1d-4031-883e-f550f549bc74"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity checks passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text I/O and Cleaning Utilities\n",
        "# ========================================\n",
        "\n",
        "# Defines reusable helpers for:\n",
        "  # • File I/O for plain-text corpora (`load_doc`, `save_doc`)\n",
        "  # • Document tokenization for large files (`clean_doc`)\n",
        "  # • Social-text normalization for short messages (tweets/posts) via\n",
        "    # explicit, testable steps (`strip_html`, `strip_urls`, `strip_emails`,\n",
        "    # `keep_letters_only`, `remove_roman_numerals`, `normalize_whitespace`,\n",
        "    # high-level `clean_social_text`, corpus-level `clean_social_corpus`,\n",
        "    # and a simple whitespace tokenizer `tokenize_simple`)\n",
        "  # • Environment detection for a writable sample-data directory (`SAMPLE_DIR`)\n",
        "    # supporting both local Jupyter and Google Colab patterns.\n",
        "\n",
        "# --- Social text cleaning helpers (fits alongside load_doc / clean_doc / save_doc) ---\n",
        "import re\n",
        "from typing import List, Iterable\n",
        "\n",
        "# Optional progress bar; falls back to a no-op if tqdm isn't available\n",
        "try:\n",
        "    from tqdm.auto import tqdm  # type: ignore\n",
        "except Exception:  # pragma: no cover\n",
        "    def tqdm(x):  # type: ignore\n",
        "        return x\n",
        "\n",
        "# Pre-compile patterns once\n",
        "_HTML_TAGS_RE   = re.compile(r\"<.*?>\")\n",
        "_URL_RE         = re.compile(r\"https?://\\S+|www\\.\\S+\", flags=re.IGNORECASE)\n",
        "_EMAIL_RE = re.compile(\n",
        "    r'\\b(?:mailto:)?(?:at\\s+)?[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b',\n",
        "    flags=re.IGNORECASE,\n",
        ")\n",
        "_NON_LETTERS_RE = re.compile(r\"[^A-Za-z]+\")   # ASCII letters only; see note below\n",
        "_ROMAN_RE       = re.compile(r\"\\b[MDCLXVI]+\\b\\.?\", flags=re.IGNORECASE)\n",
        "_WW_RE          = re.compile(r\"ww+\", flags=re.IGNORECASE)  # catch stray 'www' fragments\n",
        "_WS_RE          = re.compile(r\"\\s+\")\n",
        "\n",
        "def strip_html(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove HTML tags from text.\n",
        "    \"\"\"\n",
        "    return _HTML_TAGS_RE.sub(\"\", text)\n",
        "\n",
        "def strip_urls(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove URLs (http[s]:// and bare www.*) from text.\n",
        "    \"\"\"\n",
        "    return _URL_RE.sub(\"\", text)\n",
        "\n",
        "def strip_emails(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove email addresses and a preceding 'at ' if present.\n",
        "\n",
        "    Examples:\n",
        "        'Email me at jane@x.com'  -> 'Email me'\n",
        "        'Contact jane@x.com now'  -> 'Contact now'\n",
        "    \"\"\"\n",
        "    return _EMAIL_RE.sub(\"\", text)\n",
        "\n",
        "\n",
        "def remove_roman_numerals(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove standalone Roman numerals (I, IV, XIV, etc.), optionally with trailing period.\n",
        "    \"\"\"\n",
        "    return _ROMAN_RE.sub(\"\", text)\n",
        "\n",
        "\n",
        "def keep_letters_only(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replace any non-letter character with a space (A–Z only).\n",
        "    Note: this strips digits, punctuation, emojis, and diacritics.\n",
        "    \"\"\"\n",
        "    return _NON_LETTERS_RE.sub(\" \", text)\n",
        "\n",
        "def normalize_whitespace(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Collapse multiple spaces/newlines to a single space and trim edges.\n",
        "    \"\"\"\n",
        "    return _WS_RE.sub(\" \", text).strip()\n",
        "\n",
        "def clean_social_text(text: str, *, letters_only: bool = True, lowercase: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Clean a single social post/message.\n",
        "\n",
        "    Pipeline:\n",
        "      1) Strip HTML tags\n",
        "      2) Remove URLs\n",
        "      3) Remove emails\n",
        "      4) (Optional) keep only letters (A-Z), replacing others with spaces\n",
        "      5) Lowercase\n",
        "      6) Remove 'www' fragments and standalone Roman numerals\n",
        "      7) Normalize whitespace\n",
        "\n",
        "    Args:\n",
        "        text: Raw input text.\n",
        "        letters_only: If True, drop non-letters (digits, punctuation, emojis).\n",
        "        lowercase: If True, lowercase the text.\n",
        "\n",
        "    Returns:\n",
        "        Cleaned text as a single string.\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "\n",
        "    x = strip_html(text)\n",
        "    x = strip_urls(x)\n",
        "    x = strip_emails(x)\n",
        "    if letters_only:\n",
        "        x = keep_letters_only(x)\n",
        "    if lowercase:\n",
        "        x = x.lower()\n",
        "    # Misc cleanups mirroring your original intent\n",
        "    x = _WW_RE.sub(\"\", x)           # remove leftover www/ww fragments\n",
        "    x = remove_roman_numerals(x)    # drop roman numerals like 'XIV'\n",
        "    x = normalize_whitespace(x)\n",
        "    return x\n",
        "\n",
        "def tokenize_simple(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Basic whitespace tokenizer for already-cleaned text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    return text.split()\n",
        "\n",
        "def clean_social_corpus(\n",
        "    texts: Iterable[str],\n",
        "    *,\n",
        "    to_tokens: bool = False,\n",
        "    show_progress: bool = True,\n",
        "    letters_only: bool = True,\n",
        "    lowercase: bool = True,\n",
        ") -> List[List[str]] | List[str]:\n",
        "    \"\"\"\n",
        "    Clean a collection of social texts and optionally tokenize.\n",
        "\n",
        "    Args:\n",
        "        texts: Iterable of raw texts (e.g., tweets, comments).\n",
        "        to_tokens: If True, return List[List[str]] (tokens per text). If False, return cleaned strings.\n",
        "        show_progress: If True, show a progress bar when tqdm is available.\n",
        "        letters_only: Keep only letters (A-Z) before tokenization.\n",
        "        lowercase: Lowercase text before tokenization.\n",
        "\n",
        "    Returns:\n",
        "        If to_tokens is False: List[str] of cleaned strings.\n",
        "        If to_tokens is True:  List[List[str]] of tokenized strings per input text.\n",
        "    \"\"\"\n",
        "    it = tqdm(texts) if show_progress else texts\n",
        "    if to_tokens:\n",
        "        return [tokenize_simple(clean_social_text(t, letters_only=letters_only, lowercase=lowercase)) for t in it]\n",
        "    else:\n",
        "        return [clean_social_text(t, letters_only=letters_only, lowercase=lowercase) for t in it]\n",
        "\n"
      ],
      "metadata": {
        "id": "G94R4QrkPteS"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example corpus (replace with your own list of tweets/messages)\n",
        "\n",
        "raw_texts = [\n",
        "    \"<p>Check this out: https://example.com GREAT DEAL!!!</p>\",\n",
        "    \"Email me at John.Doe@example.org or visit www.mysite.org\",\n",
        "    \"We met on XIV. It was fun :)\",\n",
        "    \"Hello—World! New\\nline\\tand\\ttabs.\",\n",
        "]\n",
        "\n",
        "cleaned = clean_social_corpus(raw_texts, to_tokens=False, show_progress=False)\n",
        "tokenized = clean_social_corpus(raw_texts, to_tokens=True, show_progress=False)\n",
        "\n",
        "print(\"Cleaned strings:\")\n",
        "for s in cleaned:\n",
        "    print(\"  \", s)\n",
        "\n",
        "print(\"\\nTokenized (per text):\")\n",
        "for toks in tokenized:\n",
        "    print(\"  \", toks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2BZYdc57K8T",
        "outputId": "77b29b52-609f-48a4-8aec-ad04a30b7547"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned strings:\n",
            "   check this out great deal\n",
            "   email me or visit\n",
            "   we met on it was fun\n",
            "   hello world new line and tabs\n",
            "\n",
            "Tokenized (per text):\n",
            "   ['check', 'this', 'out', 'great', 'deal']\n",
            "   ['email', 'me', 'or', 'visit']\n",
            "   ['we', 'met', 'on', 'it', 'was', 'fun']\n",
            "   ['hello', 'world', 'new', 'line', 'and', 'tabs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity tests to catch regressions quickly\n",
        "\n",
        "def _assert_equal(a, b, msg=\"\"):\n",
        "    assert a == b, f\"{msg}\\nExpected: {b}\\nActual:   {a}\"\n",
        "\n",
        "# 1) URL & HTML stripping\n",
        "sample1 = \"<b>Deal</b> at https://x.y/z and www.foo.com\"\n",
        "out1 = clean_social_text(sample1)\n",
        "_assert_equal(out1, \"deal at and\", \"URL/HTML removal failed\")\n",
        "\n",
        "# 2) Email removal\n",
        "sample2 = \"Contact a@b.co now! or A.B-c_d@domain.io later.\"\n",
        "out2 = clean_social_text(sample2)\n",
        "_assert_equal(out2, \"contact now or later\", \"Email removal failed\")\n",
        "\n",
        "# 3) Roman numerals dropping\n",
        "sample3 = \"This is Chapter XIV. And Section vi.\"\n",
        "out3 = clean_social_text(sample3)\n",
        "_assert_equal(out3, \"this is chapter and section\", \"Roman numeral removal failed\")\n",
        "\n",
        "# 4) Letters-only + whitespace normalization\n",
        "sample4 = \"Hello—World! New\\nline\\tand\\ttabs. #hashtag 123\"\n",
        "out4 = clean_social_text(sample4)\n",
        "_assert_equal(out4, \"hello world new line and tabs hashtag\", \"Letters-only/whitespace failed\")\n",
        "\n",
        "# 5) Corpus path (clean strings)\n",
        "raws = [\"Email me: joe@x.com\", \"Visit <i>www.example.com</i> TODAY!!\"]\n",
        "cleaned = clean_social_corpus(raws, to_tokens=False, show_progress=False)\n",
        "_assert_equal(cleaned, [\"email me\", \"visit today\"], \"Corpus cleaning failed\")\n",
        "\n",
        "# 6) Corpus path (tokens)\n",
        "tokenized = clean_social_corpus(raws, to_tokens=True, show_progress=False)\n",
        "_assert_equal(tokenized, [[\"email\", \"me\"], [\"visit\", \"today\"]], \"Corpus tokenization failed\")\n",
        "\n",
        "print(\"All social-text cleaning tests passed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxkAl_Ui8WKo",
        "outputId": "f0f38e28-5a15-4b64-ab50-341f2283130b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All social-text cleaning tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement littleCleaning function to filter sentences by length\n",
        "\n",
        "def littleCleaning(sentences):\n",
        "    print(\"Starting cleaning Process\")\n",
        "    ret_list = []\n",
        "    for sentence in sentences:\n",
        "      words = sentence.split(\" \")\n",
        "      if len(words) > 5:\n",
        "        ret_list.append(sentence)\n",
        "      else:\n",
        "        continue\n",
        "    return ret_list"
      ],
      "metadata": {
        "id": "BIIggh2iPFwf"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data files (wordnet, punkt)\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrM8mQM54ZHv",
        "outputId": "d4ccb753-dd57-407a-fb1f-84a51cc70e2c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess 'republic.txt' corpus\n",
        "\n",
        "path = '/content/sample_data/republic.txt'\n",
        "text = open(path).read().lower()\n",
        "print('length of the corpus is: :', len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIiPUQaSPpwl",
        "outputId": "c23925b3-cab7-40aa-ab89-a43b357d258e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the corpus is: : 1174387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the data into lists\n",
        "\n",
        "data_list = text.split(\".\")\n",
        "data_list[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8O3n_dMQTRO",
        "outputId": "d4497363-718f-4f3d-d146-35e41874d17c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of the republic, by plato\\n\\nthis ebook is for the use of anyone anywhere in the united states and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever',\n",
              " ' you may copy it, give it away or re-use it under the terms\\nof the project gutenberg license included with this ebook or online at\\nwww',\n",
              " 'gutenberg',\n",
              " 'org',\n",
              " ' if you are not located in the united states, you\\nwill have to check the laws of the country where you are located before\\nusing this ebook',\n",
              " '\\n\\ntitle: the republic\\n\\nauthor: plato\\n\\ntranslator: b',\n",
              " ' jowett\\n\\nrelease date: october, 1998 [ebook #1497]\\n[most recently updated: september 11, 2021]\\n\\nlanguage: english\\n\\n\\nproduced by: sue asscher and david widger\\n\\n*** start of the project gutenberg ebook the republic ***\\n\\n\\n\\n\\nthe republic\\n\\nby plato\\n\\ntranslated by benjamin jowett\\n\\nnote: see also “the republic” by plato, jowett, ebook #150\\n\\n\\ncontents\\n\\n introduction and analysis',\n",
              " '\\n the republic',\n",
              " '\\n persons of the dialogue',\n",
              " '\\n book i',\n",
              " '\\n book ii',\n",
              " '\\n book iii',\n",
              " '\\n book iv',\n",
              " '\\n book v',\n",
              " '\\n book vi',\n",
              " '\\n book vii',\n",
              " '\\n book viii',\n",
              " '\\n book ix',\n",
              " '\\n book x',\n",
              " '\\n\\n\\n\\n\\n introduction and analysis']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Normalization pipeline that uses social-text utilities ---\n",
        "\n",
        "from typing import Callable, Iterable, List, Union\n",
        "\n",
        "def normalization_pipeline(\n",
        "    texts: Iterable[str],\n",
        "    *,\n",
        "    to_tokens: bool = False,\n",
        "    postprocess: Callable[[List[str]], List[str]] | None = None,\n",
        "    show_progress: bool = True,\n",
        "    letters_only: bool = True,\n",
        "    lowercase: bool = True,\n",
        ") -> Union[List[str], List[List[str]]]:\n",
        "    \"\"\"\n",
        "    Normalize a collection of short texts using Cell 1 social-text utilities.\n",
        "\n",
        "    - Uses `clean_social_corpus` for HTML/URL/email stripping, letters-only, lowercasing,\n",
        "      roman-numeral removal, and whitespace normalization.\n",
        "    - Returns strings by default (`to_tokens=False`) or tokens (`to_tokens=True`).\n",
        "    - Will only apply `postprocess` if you pass it explicitly.\n",
        "    \"\"\"\n",
        "    print(\"Starting Normalization Process\")\n",
        "    cleaned_or_tokens = clean_social_corpus(\n",
        "        texts,\n",
        "        to_tokens=to_tokens,\n",
        "        show_progress=show_progress,\n",
        "        letters_only=letters_only,\n",
        "        lowercase=lowercase,\n",
        "    )\n",
        "\n",
        "    # Only apply postprocess if explicitly provided\n",
        "    if callable(postprocess):\n",
        "        if to_tokens:\n",
        "            # If your postprocess expects strings, join first.\n",
        "            try:\n",
        "                joined = [\" \".join(toks) for toks in cleaned_or_tokens]  # type: ignore[arg-type]\n",
        "                maybe = postprocess(joined)\n",
        "                cleaned_or_tokens = maybe if maybe is not None else joined  # type: ignore[assignment]\n",
        "            except Exception as e:\n",
        "                raise TypeError(\n",
        "                    \"Postprocess failed on tokenized data. \"\n",
        "                    \"Provide a postprocess that accepts List[List[str]] or join tokens yourself.\"\n",
        "                ) from e\n",
        "        else:\n",
        "            maybe = postprocess(cleaned_or_tokens)  # type: ignore[arg-type]\n",
        "            # Guard against in-place functions that return None\n",
        "            if maybe is not None:\n",
        "                cleaned_or_tokens = maybe  # type: ignore[assignment]\n",
        "\n",
        "    print(\"Normalization Process Finished\")\n",
        "    return cleaned_or_tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "uaLOvlc-Q2Pb"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pro_sentences: list of cleaned strings (default, matches your previous pipeline)\n",
        "\n",
        "pro_sentences = normalization_pipeline(\n",
        "    data_list,         # your existing list of raw texts\n",
        "    to_tokens=False,   # keep strings to stay compatible with littleCleaning\n",
        "    show_progress=False\n",
        ")\n",
        "\n",
        "pro_sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiT1-_bfWwjq",
        "outputId": "157eac41-303d-4d3c-e505-397fb6256127"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Normalization Process\n",
            "Normalization Process Finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of the republic by plato this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever',\n",
              " 'you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at',\n",
              " 'gutenberg',\n",
              " 'org',\n",
              " 'if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and preprocess data list\n",
        "\n",
        "pro_tokens = normalization_pipeline(\n",
        "    data_list,\n",
        "    to_tokens=True,    # returns List[List[str]]\n",
        "    show_progress=False\n",
        ")\n",
        "pro_tokens[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN3aj-3HYD2L",
        "outputId": "f1639bef-03f2-4bd8-a503-e1a78a8a791b"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Normalization Process\n",
            "Normalization Process Finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the',\n",
              "  'project',\n",
              "  'gutenberg',\n",
              "  'ebook',\n",
              "  'of',\n",
              "  'the',\n",
              "  'republic',\n",
              "  'by',\n",
              "  'plato',\n",
              "  'this',\n",
              "  'ebook',\n",
              "  'is',\n",
              "  'for',\n",
              "  'the',\n",
              "  'use',\n",
              "  'of',\n",
              "  'anyone',\n",
              "  'anywhere',\n",
              "  'in',\n",
              "  'the',\n",
              "  'united',\n",
              "  'states',\n",
              "  'and',\n",
              "  'most',\n",
              "  'other',\n",
              "  'parts',\n",
              "  'of',\n",
              "  'the',\n",
              "  'world',\n",
              "  'at',\n",
              "  'no',\n",
              "  'cost',\n",
              "  'and',\n",
              "  'with',\n",
              "  'almost',\n",
              "  'no',\n",
              "  'restrictions',\n",
              "  'whatsoever'],\n",
              " ['you',\n",
              "  'may',\n",
              "  'copy',\n",
              "  'it',\n",
              "  'give',\n",
              "  'it',\n",
              "  'away',\n",
              "  'or',\n",
              "  're',\n",
              "  'use',\n",
              "  'it',\n",
              "  'under',\n",
              "  'the',\n",
              "  'terms',\n",
              "  'of',\n",
              "  'the',\n",
              "  'project',\n",
              "  'gutenberg',\n",
              "  'license',\n",
              "  'included',\n",
              "  'with',\n",
              "  'this',\n",
              "  'ebook',\n",
              "  'or',\n",
              "  'online',\n",
              "  'at']]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add unit tests for normalization_pipeline\n",
        "\n",
        "def _assert_equal(a, b, msg=\"\"):\n",
        "    assert a == b, f\"{msg}\\nExpected: {b}\\nActual:   {a}\"\n",
        "\n",
        "_demo = [\n",
        "    \"<b>Deal</b> at https://x.y/z and www.foo.com #promo\",\n",
        "    \"Email me at Jane.Doe@example.org ASAP — thanks!\",\n",
        "]\n",
        "\n",
        "# Strings out\n",
        "out = normalization_pipeline(_demo, to_tokens=False, show_progress=False)\n",
        "_assert_equal(out, [\"deal at and promo\", \"email me asap thanks\"], \"String normalization failed\")\n",
        "\n",
        "# Tokens out\n",
        "out_tok = normalization_pipeline(_demo, to_tokens=True, show_progress=False)\n",
        "_assert_equal(out_tok, [[\"deal\", \"at\", \"and\", \"promo\"], [\"email\", \"me\", \"asap\", \"thanks\"]], \"Token normalization failed\")\n",
        "\n",
        "print(\"Normalization pipeline tests passed.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cidoFFDkYsSD",
        "outputId": "04174380-120e-4bd1-af00-97053fc9b7be"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Normalization Process\n",
            "Normalization Process Finished\n",
            "Starting Normalization Process\n",
            "Normalization Process Finished\n",
            "Normalization pipeline tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check processed sentence count\n",
        "\n",
        "len(pro_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH7YgVAZcEuv",
        "outputId": "ce9b77ad-4277-4e15-856b-b6e97e98ccef"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7012"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Structuring the text into a paragraph\n",
        "\n",
        "dataText = \"\".join(pro_sentences[: 700])\n",
        "dataText[: 200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "_jrlLozecZIC",
        "outputId": "f1efc109-c6db-4d98-b42f-eae9acfddece"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the project gutenberg ebook of the republic by plato this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions what'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turn a doc into clean tokens\n",
        "\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "wpC38XvBdFqb"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and analyze corpus statistics\n",
        "\n",
        "tokens = clean_doc(dataText)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-K095Qndb3P",
        "outputId": "b5689346-4fd5-4803-8955-925262f28705"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'republic', 'by', 'plato', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoeveryou', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'atgutenbergorgif', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebooktitle', 'the', 'republic', 'author', 'plato', 'translator', 'bjowett', 'release', 'date', 'october', 'ebook', 'most', 'recently', 'updated', 'september', 'language', 'english', 'produced', 'by', 'sue', 'asscher', 'and', 'david', 'widger', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'the', 'republic', 'the', 'republic', 'by', 'plato', 'translated', 'by', 'benjamin', 'jowett', 'note', 'see', 'also', 'the', 'republic', 'by', 'plato', 'jowett', 'ebook', 'contents', 'introduction', 'and', 'analysisthe', 'republicpersons', 'of', 'the', 'dialoguebookbookbookbookbookbookbookbookbookbookintroduction', 'and', 'analysisthe', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'themthere', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and', 'institutions', 'of', 'the', 'state', 'are', 'more', 'clearly', 'drawn']\n",
            "Total Tokens: 18383\n",
            "Unique Tokens: 3722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement sequence creation for language modeling\n",
        "\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "    # select sequence of tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # convert into a line\n",
        "    line = ' '.join(seq)\n",
        "    # store\n",
        "    sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "metadata": {
        "id": "xY_4msxZdyX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c4ea78-425d-439a-8780-0b5bc65070a8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 18332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement utility function save_doc for writing sequences to file\n",
        "\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "out_filename = '/content/sample_data/republic_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "metadata": {
        "id": "gVwLzPelgTde"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement data preparation and tokenization pipeline.\n",
        "\n",
        "# Consolidated Imports\n",
        "\n",
        "import numpy\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential # Use tensorflow namespace\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding # Use tensorflow namespace\n",
        "\n",
        "in_filename = '/content/sample_data/republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "metadata": {
        "id": "p9qVrA81iCdl"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and train Recurrent Neural Network (RNN) using Long Short-Term Memory (LSTM)\n",
        "\n",
        "# Consolidated Imports\n",
        "\n",
        "import numpy\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential # Use tensorflow namespace\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding # Use tensorflow namespace\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# The input_shape is (batch_size, sequence_length), where None allows any batch size.\n",
        "model.build(input_shape=(None, seq_length))\n",
        "\n",
        "# This will now display the parameter counts.\n",
        "print(model.summary())\n",
        "\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# fit model\n",
        "batch_size=128\n",
        "epochs=50\n",
        "model.fit(X, y, batch_size=batch_size, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ugzY55D1jAxx",
        "outputId": "eeaa51df-21de-43c1-958c-3c69507976b1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │       \u001b[38;5;34m186,150\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m20,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m2,550\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3723\u001b[0m)           │       \u001b[38;5;34m189,873\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">186,150</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,550</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3723</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">189,873</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m418,973\u001b[0m (1.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">418,973</span> (1.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m418,973\u001b[0m (1.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">418,973</span> (1.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 143ms/step - accuracy: 0.0439 - loss: 7.3374\n",
            "Epoch 2/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.0856 - loss: 6.2013\n",
            "Epoch 3/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 107ms/step - accuracy: 0.0858 - loss: 6.0859\n",
            "Epoch 4/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 108ms/step - accuracy: 0.1142 - loss: 5.9900\n",
            "Epoch 5/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.1210 - loss: 5.7933\n",
            "Epoch 6/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 118ms/step - accuracy: 0.1282 - loss: 5.6929\n",
            "Epoch 7/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.1261 - loss: 5.6631\n",
            "Epoch 8/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.1374 - loss: 5.5486\n",
            "Epoch 9/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 115ms/step - accuracy: 0.1439 - loss: 5.5004\n",
            "Epoch 10/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 109ms/step - accuracy: 0.1438 - loss: 5.4687\n",
            "Epoch 11/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 123ms/step - accuracy: 0.1530 - loss: 5.4185\n",
            "Epoch 12/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 108ms/step - accuracy: 0.1458 - loss: 5.3871\n",
            "Epoch 13/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.1512 - loss: 5.3369\n",
            "Epoch 14/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 110ms/step - accuracy: 0.1649 - loss: 5.2153\n",
            "Epoch 15/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 109ms/step - accuracy: 0.1636 - loss: 5.1867\n",
            "Epoch 16/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.1770 - loss: 5.1182\n",
            "Epoch 17/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 108ms/step - accuracy: 0.1733 - loss: 5.0947\n",
            "Epoch 18/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 112ms/step - accuracy: 0.1798 - loss: 5.0235\n",
            "Epoch 19/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 108ms/step - accuracy: 0.1793 - loss: 4.9973\n",
            "Epoch 20/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.1800 - loss: 4.9686\n",
            "Epoch 21/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 108ms/step - accuracy: 0.1810 - loss: 4.9378\n",
            "Epoch 22/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.1853 - loss: 4.9118\n",
            "Epoch 23/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 109ms/step - accuracy: 0.1885 - loss: 4.8525\n",
            "Epoch 24/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 118ms/step - accuracy: 0.1979 - loss: 4.7738\n",
            "Epoch 25/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 114ms/step - accuracy: 0.1969 - loss: 4.7746\n",
            "Epoch 26/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 108ms/step - accuracy: 0.2060 - loss: 4.7060\n",
            "Epoch 27/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 110ms/step - accuracy: 0.1988 - loss: 4.6886\n",
            "Epoch 28/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 118ms/step - accuracy: 0.2055 - loss: 4.6453\n",
            "Epoch 29/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 118ms/step - accuracy: 0.2057 - loss: 4.6356\n",
            "Epoch 30/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.2113 - loss: 4.5601\n",
            "Epoch 31/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 112ms/step - accuracy: 0.2070 - loss: 4.5590\n",
            "Epoch 32/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 114ms/step - accuracy: 0.2131 - loss: 4.5056\n",
            "Epoch 33/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.2182 - loss: 4.4425\n",
            "Epoch 34/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 115ms/step - accuracy: 0.2157 - loss: 4.4058\n",
            "Epoch 35/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 109ms/step - accuracy: 0.2147 - loss: 4.4081\n",
            "Epoch 36/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.2226 - loss: 4.3286\n",
            "Epoch 37/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 109ms/step - accuracy: 0.2236 - loss: 4.2720\n",
            "Epoch 38/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 117ms/step - accuracy: 0.2297 - loss: 4.2291\n",
            "Epoch 39/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 109ms/step - accuracy: 0.2200 - loss: 4.2393\n",
            "Epoch 40/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.2286 - loss: 4.1762\n",
            "Epoch 41/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 116ms/step - accuracy: 0.2343 - loss: 4.1226\n",
            "Epoch 42/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.2294 - loss: 4.1216\n",
            "Epoch 43/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.2341 - loss: 4.0575\n",
            "Epoch 44/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 119ms/step - accuracy: 0.2338 - loss: 4.0469\n",
            "Epoch 45/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 108ms/step - accuracy: 0.2390 - loss: 4.0084\n",
            "Epoch 46/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 110ms/step - accuracy: 0.2462 - loss: 3.9506\n",
            "Epoch 47/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 119ms/step - accuracy: 0.2383 - loss: 3.9726\n",
            "Epoch 48/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 117ms/step - accuracy: 0.2446 - loss: 3.9235\n",
            "Epoch 49/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 116ms/step - accuracy: 0.2487 - loss: 3.8648\n",
            "Epoch 50/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 117ms/step - accuracy: 0.2489 - loss: 3.8495\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a059d5592e0>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add functionality to persist next word prediction model and tokenizer\n",
        "\n",
        "# save the model to file\n",
        "\n",
        "model.save('/content/sample_data/models/nexWordPredict/nextWord.keras')\n",
        "\n",
        "# save the tokenizer\n",
        "\n",
        "dump(tokenizer, open('/content/sample_data/models/tokenizer.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "en2EwARS1gPv"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement text generation function using a trained Keras model\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict probabilities for each word\n",
        "        # yhat = model.predict_classes(encoded, verbose=0)\n",
        "        predict_x=model.predict(encoded)\n",
        "        yhat=np.argmax(predict_x,axis=1)\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)"
      ],
      "metadata": {
        "id": "IpAMwTKg6byA"
      },
      "execution_count": 73,
      "outputs": []
    }
  ]
}