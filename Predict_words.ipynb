{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKkIDbZikuNs2VmQkGDnUf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AAdewunmi/Next-Word-Prediction-Project/blob/main/Predict_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict Words!\n",
        "\n",
        "```markdown\n",
        "# Predict Words — Notebook README (Quick Guide)\n",
        "\n",
        "A compact guide to train and use a next-word text generator (Keras LSTM) directly from this notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## What this notebook does\n",
        "- **Preprocesses** a plain-text corpus (*Plato’s The Republic*, public domain) with light normalization.\n",
        "- **Builds sequences** of tokens for next-word prediction.\n",
        "- **Trains** a small Keras LSTM language model (default: `Embedding(50) → LSTM(50) → Dense`).\n",
        "- **Saves & reloads** artifacts for reuse: `nextWord.h5`, `tokenizer.pkl`, `metadata.json`, and `republic_sequences.txt`.\n",
        "- **Generates text** from a seed using greedy or sampling (temperature / top-k) decoding.\n",
        "\n",
        "---\n",
        "\n",
        "## Folder layout & key files\n",
        "By default (Colab + Drive):\n",
        "```\n",
        "\n",
        "/content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/\n",
        "data/\n",
        "republic.txt\n",
        "republic_sequences.txt\n",
        "models/\n",
        "tokenizer.pkl\n",
        "metadata.json\n",
        "nextWordPredict/\n",
        "nextWord.h5\n",
        "\n",
        "````\n",
        "> If you’re running locally, update the `DRIVE_BASE` / `PROJECT_ROOT` path constants in the notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## Requirements\n",
        "- Python 3.10+ (Colab is fine)\n",
        "- TensorFlow/Keras, NLTK, tqdm, numpy\n",
        "\n",
        "Install (if needed) and download NLTK data:\n",
        "```python\n",
        "!pip -q install tensorflow nltk tqdm\n",
        "import nltk; nltk.download('punkt'); nltk.download('stopwords')\n",
        "````\n",
        "\n",
        "---\n",
        "\n",
        "## Quick start (Colab)\n",
        "\n",
        "1. **Mount Drive**\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "```\n",
        "\n",
        "2. **Run cells in order**\n",
        "\n",
        "* **Setup & utils** → text cleaning, I/O helpers\n",
        "* **Sequence building** → creates `republic_sequences.txt`\n",
        "* **Model training** → trains & saves `nextWord.h5`, `tokenizer.pkl`, `metadata.json`\n",
        "* **Inference** → loads assets and generates text\n",
        "\n",
        "3. **Generate text** (sampling example)\n",
        "\n",
        "```python\n",
        "generated = generate_seq_sampling(\n",
        "    model, tokenizer, seq_length, seed_text,\n",
        "    n_words=30, temperature=0.9, top_k=50, repetition_penalty=1.15\n",
        ")\n",
        "print(generated)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Inputs & outputs\n",
        "\n",
        "* **Input** corpus: `data/republic.txt` (plain text).\n",
        "* **Training sequences**: built from the corpus; `seq_length` is inferred from these sequences.\n",
        "* **Seed text** for inference should match the model’s expected **sequence length − 1** tokens.\n",
        "\n",
        "  * Use the helper `pick_seed_from_sequences(...)` or supply your own seed (trim to the required length).\n",
        "* **Output**: continuation of `n_words` predicted tokens.\n",
        "\n",
        "---\n",
        "\n",
        "## Default hyperparameters (tunable)\n",
        "\n",
        "```text\n",
        "embedding_dim = 50\n",
        "lstm_units   = 50\n",
        "batch_size   = 128\n",
        "epochs       = 50\n",
        "```\n",
        "\n",
        "> For better quality, consider increasing `embedding_dim`, `lstm_units`, training data size, and epochs.\n",
        "\n",
        "---\n",
        "\n",
        "## Reuse the trained model\n",
        "\n",
        "Load artifacts and generate without retraining:\n",
        "\n",
        "```python\n",
        "model, tokenizer, seq_length, meta = load_nextword_assets(\n",
        "    model_path=MODEL_PATH, tokenizer_path=TOKENIZER_PATH, seqs_path=SEQS_PATH\n",
        ")\n",
        "seed = pick_seed_from_sequences(SEQS_PATH, seq_length)\n",
        "print(generate_seq_sampling(model, tokenizer, seq_length, seed, n_words=30))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Customization tips\n",
        "\n",
        "* **Different corpus**: replace `republic.txt`, then re-run preprocessing → training.\n",
        "* **Stricter cleaning**: adjust the normalization pipeline (URLs, HTML, casing, stopwords).\n",
        "* **Decoding style**: switch between greedy (`generate_seq`) and sampling (`generate_seq_sampling`), play with `temperature` and `top_k`.\n",
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "* **File not found**: confirm Drive is mounted and paths match your environment.\n",
        "* **Tokenizer/model mismatch**: ensure `tokenizer.pkl` and `nextWord.h5` come from the **same training run**.\n",
        "* **NLTK resource errors**: run the `nltk.download(...)` lines above.\n",
        "* **OOM / slow training**: lower `batch_size`, shorten sequences, or use a smaller model.\n",
        "\n",
        "---\n",
        "\n",
        "## Attribution\n",
        "\n",
        "* Text: *Plato — The Republic* (public domain).\n",
        "* Libraries: TensorFlow/Keras, NLTK, tqdm, numpy."
      ],
      "metadata": {
        "id": "fJR9MBr5IIYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI_jtx3WQFNb",
        "outputId": "c9a25f02-d466-48e0-b7ef-0b21bdc532e1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "478d04b6"
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import pkg_resources\n",
        "import pickle\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "\n",
        "import numpy\n",
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "WWfT8jo31nfn"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add utility functions for document handling and sample data directory detection\n",
        "\n",
        "# --- Drive-backed, project-scoped document & model utilities ---\n",
        "from pathlib import Path\n",
        "from typing import List, Union, Tuple, Optional\n",
        "import string\n",
        "import json\n",
        "import pickle\n",
        "import platform\n",
        "import time\n",
        "\n",
        "# 0) Drive mount (must be done in a previous cell)\n",
        "DRIVE_MOUNT = Path(\"/content/drive\")\n",
        "DRIVE_MYDRIVE = DRIVE_MOUNT / \"MyDrive\"\n",
        "\n",
        "def _require_drive() -> None:\n",
        "    \"\"\"Raise if Google Drive isn't mounted in Colab.\"\"\"\n",
        "    if not DRIVE_MYDRIVE.exists():\n",
        "        raise RuntimeError(\n",
        "            \"Google Drive is not mounted at /content/drive. \"\n",
        "            \"Run: from google.colab import drive; drive.mount('/content/drive')\"\n",
        "        )\n",
        "\n",
        "def _project_root_dir() -> Path:\n",
        "    \"\"\"\n",
        "    Project root inside MyDrive.\n",
        "    Uses the exact folder you specified:\n",
        "      MyDrive/Colab Notebooks/Predict-Words-Analysis\n",
        "    \"\"\"\n",
        "    _require_drive()\n",
        "    root = DRIVE_MYDRIVE / \"Colab Notebooks\" / \"Predict-Words-Analysis\"\n",
        "    root.mkdir(parents=True, exist_ok=True)\n",
        "    return root\n",
        "\n",
        "# 1) Project-scoped directories\n",
        "PROJECT_ROOT = _project_root_dir()\n",
        "DATA_DIR     = PROJECT_ROOT / \"data\"                 # *.txt\n",
        "MODELS_DIR   = PROJECT_ROOT / \"models\"               # *.pkl, metadata.json\n",
        "NEXTWORD_DIR = MODELS_DIR / \"nextWordPredict\"        # *.keras / *.h5\n",
        "for d in (DATA_DIR, MODELS_DIR, NEXTWORD_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _resolve_path(filename: Union[str, Path]) -> Path:\n",
        "    \"\"\"\n",
        "    Resolve a filename into the correct project folder based on extension:\n",
        "      *.txt   -> DATA_DIR\n",
        "      *.pkl   -> MODELS_DIR\n",
        "      *.keras/*.h5 -> NEXTWORD_DIR\n",
        "      otherwise -> PROJECT_ROOT\n",
        "    Absolute paths are returned as-is.\n",
        "    \"\"\"\n",
        "    p = Path(filename)\n",
        "    if p.is_absolute():\n",
        "        return p\n",
        "    suffix = p.suffix.lower()\n",
        "    if suffix == \".txt\":\n",
        "        return DATA_DIR / p.name\n",
        "    if suffix == \".pkl\":\n",
        "        return MODELS_DIR / p.name\n",
        "    if suffix in (\".keras\", \".h5\"):\n",
        "        return NEXTWORD_DIR / p.name\n",
        "    return PROJECT_ROOT / p.name\n",
        "\n",
        "# 2) Document I/O\n",
        "def load_doc(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Read a UTF-8 text file.\n",
        "    Relative names go to .../Predict-Words-Analysis/data/.\n",
        "    \"\"\"\n",
        "    path = _resolve_path(filename)\n",
        "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def clean_doc(doc: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Convert raw document text into cleaned, lowercased, alphabetic tokens.\n",
        "\n",
        "    Steps:\n",
        "      1) Replace double hyphens with a space.\n",
        "      2) Split on whitespace.\n",
        "      3) Remove ASCII punctuation from each token.\n",
        "      4) Keep only purely alphabetic tokens (isalpha()).\n",
        "      5) Lowercase all tokens.\n",
        "    \"\"\"\n",
        "    doc = doc.replace(\"--\", \" \")\n",
        "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "    tokens = [w.translate(table) for w in doc.split()]\n",
        "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
        "    return tokens\n",
        "\n",
        "def save_doc(lines: List[str], filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Save a list of strings to disk, one per line (UTF-8).\n",
        "    Relative *.txt files go to .../data/.\n",
        "    \"\"\"\n",
        "    path = _resolve_path(filename)\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "# 3) Optional: model/tokenizer persistence in your structure\n",
        "def persist_nextword_assets(model, tokenizer, *, model_name: str = \"nextWord\") -> Tuple[Path, Path, Path]:\n",
        "    \"\"\"\n",
        "    Save model (.keras) to models/nextWordPredict/, tokenizer (.pkl) and metadata.json to models/.\n",
        "    Returns (model_path, tokenizer_path, metadata_path).\n",
        "    \"\"\"\n",
        "    # Defer import so this file doesn't require TF unless you call this\n",
        "    from tensorflow.keras.models import load_model as _  # noqa: F401\n",
        "\n",
        "    NEXTWORD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    model_path = NEXTWORD_DIR / f\"{model_name}.keras\"\n",
        "    tokenizer_path = MODELS_DIR / \"tokenizer.pkl\"\n",
        "    metadata_path = MODELS_DIR / \"metadata.json\"\n",
        "\n",
        "    model.save(model_path)\n",
        "    with tokenizer_path.open(\"wb\") as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "\n",
        "    # Try to infer seq_length from model.input_shape[1] if present\n",
        "    seq_len = None\n",
        "    try:\n",
        "        ish = getattr(model, \"input_shape\", None)\n",
        "        if isinstance(ish, (list, tuple)) and len(ish) >= 2 and isinstance(ish[1], int):\n",
        "            seq_len = int(ish[1])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    meta = {\n",
        "        \"created_at\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "        \"seq_length\": seq_len,\n",
        "        \"vocab_size\": len(getattr(tokenizer, \"word_index\", {})) + 1,\n",
        "        \"python_version\": platform.python_version(),\n",
        "    }\n",
        "    with metadata_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return model_path, tokenizer_path, metadata_path\n",
        "\n",
        "def load_nextword_assets(model_name: str = \"nextWord\") -> Tuple[object, object, dict]:\n",
        "    \"\"\"\n",
        "    Load (model, tokenizer, metadata) from:\n",
        "      models/nextWordPredict/<model_name>.h5\n",
        "      models/tokenizer.pkl\n",
        "      models/metadata.json\n",
        "    \"\"\"\n",
        "    from tensorflow.keras.models import load_model\n",
        "    model_path = NEXTWORD_DIR / f\"{model_name}.h5\"\n",
        "    tokenizer_path = MODELS_DIR / \"tokenizer.pkl\"\n",
        "    metadata_path = MODELS_DIR / \"metadata.json\"\n",
        "\n",
        "    if not model_path.exists():\n",
        "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
        "    if not tokenizer_path.exists():\n",
        "        raise FileNotFoundError(f\"Tokenizer not found: {tokenizer_path}\")\n",
        "\n",
        "    model = load_model(model_path, compile=False)\n",
        "    with tokenizer_path.open(\"rb\") as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    meta = {}\n",
        "    if metadata_path.exists():\n",
        "        with metadata_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            meta = json.load(f)\n",
        "    return model, tokenizer, meta\n",
        "\n"
      ],
      "metadata": {
        "id": "f17Zepp5XICk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add script to tokenize text file from sample_data and save tokens\n",
        "\n",
        "INPUT_FILE = \"republic.txt\"\n",
        "OUTPUT_FILE = \"republic-tokenised.txt\"\n",
        "\n",
        "# Execute the pipeline\n",
        "text = load_doc(INPUT_FILE)\n",
        "tokens = clean_doc(text)\n",
        "save_doc(tokens, OUTPUT_FILE)\n",
        "\n",
        "print(f\"Read from: {INPUT_FILE}\")\n",
        "print(f\"Wrote  to: {OUTPUT_FILE}\")\n",
        "print(f\"Sample tokens: {tokens[:25]}\")\n",
        "print(f\"Total tokens: {len(tokens):,}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjlLnFTB1ptw",
        "outputId": "4d16f914-9146-4d5a-f533-6bc178ec0d63"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read from: republic.txt\n",
            "Wrote  to: republic-tokenised.txt\n",
            "Sample tokens: ['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'republic', 'by', 'plato', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other']\n",
            "Total tokens: 209,695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell — Sanity checks for Drive-backed document pipeline\n",
        "-------------------------------------------------------\n",
        "Verifies:\n",
        "  • Google Drive is mounted.\n",
        "  • `text` and `tokens` look sane.\n",
        "  • Output file exists at the resolved Drive path.\n",
        "  • save/load round-trip works in the project data folder.\n",
        "Assumes you've already run the Drive-backed helpers (with _require_drive/_resolve_path).\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Ensure Drive is mounted and project folders exist\n",
        "_require_drive()\n",
        "\n",
        "# Basic object checks (expects you already computed `text`, `tokens`, and set `OUTPUT_FILE`)\n",
        "assert isinstance(text, str) and text.strip(), \"Input text is empty or not a string.\"\n",
        "assert isinstance(tokens, list) and all(isinstance(t, str) for t in tokens), \"Tokens must be a list of strings.\"\n",
        "assert all(t.isalpha() for t in tokens), \"Non-alphabetic tokens slipped through.\"\n",
        "\n",
        "# Output file must exist in Drive (resolve relative names into your project layout)\n",
        "out_path = _resolve_path(OUTPUT_FILE)\n",
        "assert out_path.exists(), f\"Output file was not written to Drive: {out_path}\"\n",
        "\n",
        "# Round-trip write/read in Drive data folder\n",
        "tmp_out = _resolve_path(\"_tmp_tokens.txt\")   # goes to .../data/_tmp_tokens.txt\n",
        "save_doc([\"A\", \"b\", \"c\"], tmp_out)\n",
        "reloaded = load_doc(tmp_out).splitlines()\n",
        "assert reloaded == [\"A\", \"b\", \"c\"], \"save_doc/load_doc round-trip failed.\"\n",
        "tmp_out.unlink(missing_ok=True)\n",
        "\n",
        "print(\"Sanity checks (Drive) passed.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpoaejgg5a5C",
        "outputId": "e4277335-40d8-4a16-f4da-0c369e6301af"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanity checks (Drive) passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text I/O and Cleaning Utilities\n",
        "# ========================================\n",
        "\n",
        "# Defines reusable helpers for:\n",
        "  # • File I/O for plain-text corpora (`load_doc`, `save_doc`)\n",
        "  # • Document tokenization for large files (`clean_doc`)\n",
        "  # • Social-text normalization for short messages (tweets/posts) via\n",
        "    # explicit, testable steps (`strip_html`, `strip_urls`, `strip_emails`,\n",
        "    # `keep_letters_only`, `remove_roman_numerals`, `normalize_whitespace`,\n",
        "    # high-level `clean_social_text`, corpus-level `clean_social_corpus`,\n",
        "    # and a simple whitespace tokenizer `tokenize_simple`)\n",
        "  # • Environment detection for a writable sample-data directory (`SAMPLE_DIR`)\n",
        "    # supporting both local Jupyter and Google Colab patterns.\n",
        "\n",
        "# --- Social text cleaning helpers (fits alongside load_doc / clean_doc / save_doc) ---\n",
        "import re\n",
        "from typing import List, Iterable\n",
        "\n",
        "# Optional progress bar; falls back to a no-op if tqdm isn't available\n",
        "try:\n",
        "    from tqdm.auto import tqdm  # type: ignore\n",
        "except Exception:  # pragma: no cover\n",
        "    def tqdm(x):  # type: ignore\n",
        "        return x\n",
        "\n",
        "# Pre-compile patterns once\n",
        "_HTML_TAGS_RE   = re.compile(r\"<.*?>\")\n",
        "_URL_RE         = re.compile(r\"https?://\\S+|www\\.\\S+\", flags=re.IGNORECASE)\n",
        "_EMAIL_RE = re.compile(\n",
        "    r'\\b(?:mailto:)?(?:at\\s+)?[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b',\n",
        "    flags=re.IGNORECASE,\n",
        ")\n",
        "_NON_LETTERS_RE = re.compile(r\"[^A-Za-z]+\")   # ASCII letters only; see note below\n",
        "_ROMAN_RE       = re.compile(r\"\\b[MDCLXVI]+\\b\\.?\", flags=re.IGNORECASE)\n",
        "_WW_RE          = re.compile(r\"ww+\", flags=re.IGNORECASE)  # catch stray 'www' fragments\n",
        "_WS_RE          = re.compile(r\"\\s+\")\n",
        "\n",
        "def strip_html(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove HTML tags from text.\n",
        "    \"\"\"\n",
        "    return _HTML_TAGS_RE.sub(\"\", text)\n",
        "\n",
        "def strip_urls(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove URLs (http[s]:// and bare www.*) from text.\n",
        "    \"\"\"\n",
        "    return _URL_RE.sub(\"\", text)\n",
        "\n",
        "def strip_emails(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove email addresses and a preceding 'at ' if present.\n",
        "\n",
        "    Examples:\n",
        "        'Email me at jane@x.com'  -> 'Email me'\n",
        "        'Contact jane@x.com now'  -> 'Contact now'\n",
        "    \"\"\"\n",
        "    return _EMAIL_RE.sub(\"\", text)\n",
        "\n",
        "\n",
        "def remove_roman_numerals(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove standalone Roman numerals (I, IV, XIV, etc.), optionally with trailing period.\n",
        "    \"\"\"\n",
        "    return _ROMAN_RE.sub(\"\", text)\n",
        "\n",
        "\n",
        "def keep_letters_only(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replace any non-letter character with a space (A–Z only).\n",
        "    Note: this strips digits, punctuation, emojis, and diacritics.\n",
        "    \"\"\"\n",
        "    return _NON_LETTERS_RE.sub(\" \", text)\n",
        "\n",
        "def normalize_whitespace(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Collapse multiple spaces/newlines to a single space and trim edges.\n",
        "    \"\"\"\n",
        "    return _WS_RE.sub(\" \", text).strip()\n",
        "\n",
        "def clean_social_text(text: str, *, letters_only: bool = True, lowercase: bool = True) -> str:\n",
        "    \"\"\"\n",
        "    Clean a single social post/message.\n",
        "\n",
        "    Pipeline:\n",
        "      1) Strip HTML tags\n",
        "      2) Remove URLs\n",
        "      3) Remove emails\n",
        "      4) (Optional) keep only letters (A-Z), replacing others with spaces\n",
        "      5) Lowercase\n",
        "      6) Remove 'www' fragments and standalone Roman numerals\n",
        "      7) Normalize whitespace\n",
        "\n",
        "    Args:\n",
        "        text: Raw input text.\n",
        "        letters_only: If True, drop non-letters (digits, punctuation, emojis).\n",
        "        lowercase: If True, lowercase the text.\n",
        "\n",
        "    Returns:\n",
        "        Cleaned text as a single string.\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "\n",
        "    x = strip_html(text)\n",
        "    x = strip_urls(x)\n",
        "    x = strip_emails(x)\n",
        "    if letters_only:\n",
        "        x = keep_letters_only(x)\n",
        "    if lowercase:\n",
        "        x = x.lower()\n",
        "    # Misc cleanups mirroring your original intent\n",
        "    x = _WW_RE.sub(\"\", x)           # remove leftover www/ww fragments\n",
        "    x = remove_roman_numerals(x)    # drop roman numerals like 'XIV'\n",
        "    x = normalize_whitespace(x)\n",
        "    return x\n",
        "\n",
        "def tokenize_simple(text: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Basic whitespace tokenizer for already-cleaned text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    return text.split()\n",
        "\n",
        "def clean_social_corpus(\n",
        "    texts: Iterable[str],\n",
        "    *,\n",
        "    to_tokens: bool = False,\n",
        "    show_progress: bool = True,\n",
        "    letters_only: bool = True,\n",
        "    lowercase: bool = True,\n",
        ") -> List[List[str]] | List[str]:\n",
        "    \"\"\"\n",
        "    Clean a collection of social texts and optionally tokenize.\n",
        "\n",
        "    Args:\n",
        "        texts: Iterable of raw texts (e.g., tweets, comments).\n",
        "        to_tokens: If True, return List[List[str]] (tokens per text). If False, return cleaned strings.\n",
        "        show_progress: If True, show a progress bar when tqdm is available.\n",
        "        letters_only: Keep only letters (A-Z) before tokenization.\n",
        "        lowercase: Lowercase text before tokenization.\n",
        "\n",
        "    Returns:\n",
        "        If to_tokens is False: List[str] of cleaned strings.\n",
        "        If to_tokens is True:  List[List[str]] of tokenized strings per input text.\n",
        "    \"\"\"\n",
        "    it = tqdm(texts) if show_progress else texts\n",
        "    if to_tokens:\n",
        "        return [tokenize_simple(clean_social_text(t, letters_only=letters_only, lowercase=lowercase)) for t in it]\n",
        "    else:\n",
        "        return [clean_social_text(t, letters_only=letters_only, lowercase=lowercase) for t in it]\n",
        "\n"
      ],
      "metadata": {
        "id": "G94R4QrkPteS"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example corpus (replace with your own list of tweets/messages)\n",
        "\n",
        "raw_texts = [\n",
        "    \"<p>Check this out: https://example.com GREAT DEAL!!!</p>\",\n",
        "    \"Email me at John.Doe@example.org or visit www.mysite.org\",\n",
        "    \"We met on XIV. It was fun :)\",\n",
        "    \"Hello—World! New\\nline\\tand\\ttabs.\",\n",
        "]\n",
        "\n",
        "cleaned = clean_social_corpus(raw_texts, to_tokens=False, show_progress=False)\n",
        "tokenized = clean_social_corpus(raw_texts, to_tokens=True, show_progress=False)\n",
        "\n",
        "print(\"Cleaned strings:\")\n",
        "for s in cleaned:\n",
        "    print(\"  \", s)\n",
        "\n",
        "print(\"\\nTokenized (per text):\")\n",
        "for toks in tokenized:\n",
        "    print(\"  \", toks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2BZYdc57K8T",
        "outputId": "43356da0-5fbe-40b0-acd6-f699a30d3261"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned strings:\n",
            "   check this out great deal\n",
            "   email me or visit\n",
            "   we met on it was fun\n",
            "   hello world new line and tabs\n",
            "\n",
            "Tokenized (per text):\n",
            "   ['check', 'this', 'out', 'great', 'deal']\n",
            "   ['email', 'me', 'or', 'visit']\n",
            "   ['we', 'met', 'on', 'it', 'was', 'fun']\n",
            "   ['hello', 'world', 'new', 'line', 'and', 'tabs']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity tests to catch regressions quickly\n",
        "\n",
        "def _assert_equal(a, b, msg=\"\"):\n",
        "    assert a == b, f\"{msg}\\nExpected: {b}\\nActual:   {a}\"\n",
        "\n",
        "# 1) URL & HTML stripping\n",
        "sample1 = \"<b>Deal</b> at https://x.y/z and www.foo.com\"\n",
        "out1 = clean_social_text(sample1)\n",
        "_assert_equal(out1, \"deal at and\", \"URL/HTML removal failed\")\n",
        "\n",
        "# 2) Email removal\n",
        "sample2 = \"Contact a@b.co now! or A.B-c_d@domain.io later.\"\n",
        "out2 = clean_social_text(sample2)\n",
        "_assert_equal(out2, \"contact now or later\", \"Email removal failed\")\n",
        "\n",
        "# 3) Roman numerals dropping\n",
        "sample3 = \"This is Chapter XIV. And Section vi.\"\n",
        "out3 = clean_social_text(sample3)\n",
        "_assert_equal(out3, \"this is chapter and section\", \"Roman numeral removal failed\")\n",
        "\n",
        "# 4) Letters-only + whitespace normalization\n",
        "sample4 = \"Hello—World! New\\nline\\tand\\ttabs. #hashtag 123\"\n",
        "out4 = clean_social_text(sample4)\n",
        "_assert_equal(out4, \"hello world new line and tabs hashtag\", \"Letters-only/whitespace failed\")\n",
        "\n",
        "# 5) Corpus path (clean strings)\n",
        "raws = [\"Email me: joe@x.com\", \"Visit <i>www.example.com</i> TODAY!!\"]\n",
        "cleaned = clean_social_corpus(raws, to_tokens=False, show_progress=False)\n",
        "_assert_equal(cleaned, [\"email me\", \"visit today\"], \"Corpus cleaning failed\")\n",
        "\n",
        "# 6) Corpus path (tokens)\n",
        "tokenized = clean_social_corpus(raws, to_tokens=True, show_progress=False)\n",
        "_assert_equal(tokenized, [[\"email\", \"me\"], [\"visit\", \"today\"]], \"Corpus tokenization failed\")\n",
        "\n",
        "print(\"All social-text cleaning tests passed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxkAl_Ui8WKo",
        "outputId": "221c1bf3-5cb3-499a-f35b-1776370d89d8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All social-text cleaning tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement littleCleaning function to filter sentences by length\n",
        "\n",
        "def littleCleaning(sentences):\n",
        "    print(\"Starting cleaning Process\")\n",
        "    ret_list = []\n",
        "    for sentence in sentences:\n",
        "      words = sentence.split(\" \")\n",
        "      if len(words) > 5:\n",
        "        ret_list.append(sentence)\n",
        "      else:\n",
        "        continue\n",
        "    return ret_list"
      ],
      "metadata": {
        "id": "BIIggh2iPFwf"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data files (wordnet, punkt)\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrM8mQM54ZHv",
        "outputId": "8cc8e2f3-c20d-4109-fa0a-e0e04a091682"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess 'republic.txt' corpus\n",
        "\n",
        "# Uses the Drive-backed helpers set up earlier.\n",
        "\n",
        "text = load_doc(\"republic.txt\").lower()   # resolves to /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/data/republic.txt\n",
        "print(f\"length of the corpus: {len(text):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIiPUQaSPpwl",
        "outputId": "fcd33d46-4d96-497f-cb4f-f799f857c97c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the corpus: 1,174,387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the data into lists\n",
        "\n",
        "data_list = text.split(\".\")\n",
        "data_list[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8O3n_dMQTRO",
        "outputId": "64834504-0e60-4b91-9515-bbb2ed23c423"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of the republic, by plato\\n\\nthis ebook is for the use of anyone anywhere in the united states and\\nmost other parts of the world at no cost and with almost no restrictions\\nwhatsoever',\n",
              " ' you may copy it, give it away or re-use it under the terms\\nof the project gutenberg license included with this ebook or online at\\nwww',\n",
              " 'gutenberg',\n",
              " 'org',\n",
              " ' if you are not located in the united states, you\\nwill have to check the laws of the country where you are located before\\nusing this ebook',\n",
              " '\\n\\ntitle: the republic\\n\\nauthor: plato\\n\\ntranslator: b',\n",
              " ' jowett\\n\\nrelease date: october, 1998 [ebook #1497]\\n[most recently updated: september 11, 2021]\\n\\nlanguage: english\\n\\n\\nproduced by: sue asscher and david widger\\n\\n*** start of the project gutenberg ebook the republic ***\\n\\n\\n\\n\\nthe republic\\n\\nby plato\\n\\ntranslated by benjamin jowett\\n\\nnote: see also “the republic” by plato, jowett, ebook #150\\n\\n\\ncontents\\n\\n introduction and analysis',\n",
              " '\\n the republic',\n",
              " '\\n persons of the dialogue',\n",
              " '\\n book i',\n",
              " '\\n book ii',\n",
              " '\\n book iii',\n",
              " '\\n book iv',\n",
              " '\\n book v',\n",
              " '\\n book vi',\n",
              " '\\n book vii',\n",
              " '\\n book viii',\n",
              " '\\n book ix',\n",
              " '\\n book x',\n",
              " '\\n\\n\\n\\n\\n introduction and analysis']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Normalization pipeline that uses social-text utilities ---\n",
        "\n",
        "from typing import Callable, Iterable, List, Union\n",
        "\n",
        "def normalization_pipeline(\n",
        "    texts: Iterable[str],\n",
        "    *,\n",
        "    to_tokens: bool = False,\n",
        "    postprocess: Callable[[List[str]], List[str]] | None = None,\n",
        "    show_progress: bool = True,\n",
        "    letters_only: bool = True,\n",
        "    lowercase: bool = True,\n",
        ") -> Union[List[str], List[List[str]]]:\n",
        "    \"\"\"\n",
        "    Normalize a collection of short texts using Cell 1 social-text utilities.\n",
        "\n",
        "    - Uses `clean_social_corpus` for HTML/URL/email stripping, letters-only, lowercasing,\n",
        "      roman-numeral removal, and whitespace normalization.\n",
        "    - Returns strings by default (`to_tokens=False`) or tokens (`to_tokens=True`).\n",
        "    - Will only apply `postprocess` if you pass it explicitly.\n",
        "    \"\"\"\n",
        "    print(\"Starting Normalization Process\")\n",
        "    cleaned_or_tokens = clean_social_corpus(\n",
        "        texts,\n",
        "        to_tokens=to_tokens,\n",
        "        show_progress=show_progress,\n",
        "        letters_only=letters_only,\n",
        "        lowercase=lowercase,\n",
        "    )\n",
        "\n",
        "    # Only apply postprocess if explicitly provided\n",
        "    if callable(postprocess):\n",
        "        if to_tokens:\n",
        "            # If your postprocess expects strings, join first.\n",
        "            try:\n",
        "                joined = [\" \".join(toks) for toks in cleaned_or_tokens]  # type: ignore[arg-type]\n",
        "                maybe = postprocess(joined)\n",
        "                cleaned_or_tokens = maybe if maybe is not None else joined  # type: ignore[assignment]\n",
        "            except Exception as e:\n",
        "                raise TypeError(\n",
        "                    \"Postprocess failed on tokenized data. \"\n",
        "                    \"Provide a postprocess that accepts List[List[str]] or join tokens yourself.\"\n",
        "                ) from e\n",
        "        else:\n",
        "            maybe = postprocess(cleaned_or_tokens)  # type: ignore[arg-type]\n",
        "            # Guard against in-place functions that return None\n",
        "            if maybe is not None:\n",
        "                cleaned_or_tokens = maybe  # type: ignore[assignment]\n",
        "\n",
        "    print(\"Normalization Process Finished\")\n",
        "    return cleaned_or_tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "uaLOvlc-Q2Pb"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pro_sentences: list of cleaned strings (default, matches your previous pipeline)\n",
        "\n",
        "pro_sentences = normalization_pipeline(\n",
        "    data_list,         # your existing list of raw texts\n",
        "    to_tokens=False,   # keep strings to stay compatible with littleCleaning\n",
        "    show_progress=False\n",
        ")\n",
        "\n",
        "pro_sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiT1-_bfWwjq",
        "outputId": "eaf492fe-8c45-49a5-b7c5-869497b06b7e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Normalization Process\n",
            "Normalization Process Finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of the republic by plato this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever',\n",
              " 'you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at',\n",
              " 'gutenberg',\n",
              " 'org',\n",
              " 'if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and preprocess data list\n",
        "\n",
        "pro_tokens = normalization_pipeline(\n",
        "    data_list,\n",
        "    to_tokens=True,    # returns List[List[str]]\n",
        "    show_progress=False\n",
        ")\n",
        "pro_tokens[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SN3aj-3HYD2L",
        "outputId": "707ef856-b28c-4f37-b6d9-84b6eb41f7c2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Normalization Process\n",
            "Normalization Process Finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['the',\n",
              "  'project',\n",
              "  'gutenberg',\n",
              "  'ebook',\n",
              "  'of',\n",
              "  'the',\n",
              "  'republic',\n",
              "  'by',\n",
              "  'plato',\n",
              "  'this',\n",
              "  'ebook',\n",
              "  'is',\n",
              "  'for',\n",
              "  'the',\n",
              "  'use',\n",
              "  'of',\n",
              "  'anyone',\n",
              "  'anywhere',\n",
              "  'in',\n",
              "  'the',\n",
              "  'united',\n",
              "  'states',\n",
              "  'and',\n",
              "  'most',\n",
              "  'other',\n",
              "  'parts',\n",
              "  'of',\n",
              "  'the',\n",
              "  'world',\n",
              "  'at',\n",
              "  'no',\n",
              "  'cost',\n",
              "  'and',\n",
              "  'with',\n",
              "  'almost',\n",
              "  'no',\n",
              "  'restrictions',\n",
              "  'whatsoever'],\n",
              " ['you',\n",
              "  'may',\n",
              "  'copy',\n",
              "  'it',\n",
              "  'give',\n",
              "  'it',\n",
              "  'away',\n",
              "  'or',\n",
              "  're',\n",
              "  'use',\n",
              "  'it',\n",
              "  'under',\n",
              "  'the',\n",
              "  'terms',\n",
              "  'of',\n",
              "  'the',\n",
              "  'project',\n",
              "  'gutenberg',\n",
              "  'license',\n",
              "  'included',\n",
              "  'with',\n",
              "  'this',\n",
              "  'ebook',\n",
              "  'or',\n",
              "  'online',\n",
              "  'at']]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add unit tests for normalization_pipeline\n",
        "\n",
        "def _assert_equal(a, b, msg=\"\"):\n",
        "    assert a == b, f\"{msg}\\nExpected: {b}\\nActual:   {a}\"\n",
        "\n",
        "_demo = [\n",
        "    \"<b>Deal</b> at https://x.y/z and www.foo.com #promo\",\n",
        "    \"Email me at Jane.Doe@example.org ASAP — thanks!\",\n",
        "]\n",
        "\n",
        "# Strings out\n",
        "out = normalization_pipeline(_demo, to_tokens=False, show_progress=False)\n",
        "_assert_equal(out, [\"deal at and promo\", \"email me asap thanks\"], \"String normalization failed\")\n",
        "\n",
        "# Tokens out\n",
        "out_tok = normalization_pipeline(_demo, to_tokens=True, show_progress=False)\n",
        "_assert_equal(out_tok, [[\"deal\", \"at\", \"and\", \"promo\"], [\"email\", \"me\", \"asap\", \"thanks\"]], \"Token normalization failed\")\n",
        "\n",
        "print(\"Normalization pipeline tests passed.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cidoFFDkYsSD",
        "outputId": "e1b44af6-892b-4596-9dc3-a1092cc2ba52"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Normalization Process\n",
            "Normalization Process Finished\n",
            "Starting Normalization Process\n",
            "Normalization Process Finished\n",
            "Normalization pipeline tests passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check processed sentence count\n",
        "\n",
        "len(pro_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH7YgVAZcEuv",
        "outputId": "2a68cce5-4ab0-472f-adfc-2007afcaf667"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7012"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Structuring the text into a paragraph\n",
        "\n",
        "dataText = \"\".join(pro_sentences[: 700])\n",
        "dataText[: 200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_jrlLozecZIC",
        "outputId": "6738f82d-b83f-4dae-d10b-8ef8bd13a7a9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the project gutenberg ebook of the republic by plato this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions what'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# turn a doc into clean tokens\n",
        "\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "wpC38XvBdFqb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and analyze corpus statistics\n",
        "\n",
        "tokens = clean_doc(dataText)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-K095Qndb3P",
        "outputId": "8107185e-4b8e-4658-a93e-17aed5605266"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'republic', 'by', 'plato', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoeveryou', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'atgutenbergorgif', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebooktitle', 'the', 'republic', 'author', 'plato', 'translator', 'bjowett', 'release', 'date', 'october', 'ebook', 'most', 'recently', 'updated', 'september', 'language', 'english', 'produced', 'by', 'sue', 'asscher', 'and', 'david', 'widger', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'the', 'republic', 'the', 'republic', 'by', 'plato', 'translated', 'by', 'benjamin', 'jowett', 'note', 'see', 'also', 'the', 'republic', 'by', 'plato', 'jowett', 'ebook', 'contents', 'introduction', 'and', 'analysisthe', 'republicpersons', 'of', 'the', 'dialoguebookbookbookbookbookbookbookbookbookbookintroduction', 'and', 'analysisthe', 'republic', 'of', 'plato', 'is', 'the', 'longest', 'of', 'his', 'works', 'with', 'the', 'exception', 'of', 'the', 'laws', 'and', 'is', 'certainly', 'the', 'greatest', 'of', 'themthere', 'are', 'nearer', 'approaches', 'to', 'modern', 'metaphysics', 'in', 'the', 'philebus', 'and', 'in', 'the', 'sophist', 'the', 'politicus', 'or', 'statesman', 'is', 'more', 'ideal', 'the', 'form', 'and', 'institutions', 'of', 'the', 'state', 'are', 'more', 'clearly', 'drawn']\n",
            "Total Tokens: 18383\n",
            "Unique Tokens: 3722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement sequence creation for language modeling\n",
        "\n",
        "length = 50 + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "    # select sequence of tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # convert into a line\n",
        "    line = ' '.join(seq)\n",
        "    # store\n",
        "    sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "metadata": {
        "id": "xY_4msxZdyX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab6fb1e-d32b-4eb2-ee6e-9c21c43f71a0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 18332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement utility function save_doc for writing sequences to file\n",
        "\n",
        "# --- Persist training sequences to Drive using the project helpers ---\n",
        "\n",
        "# If sequences are already strings, this is a no-op; if they are lists/tuples of tokens,\n",
        "# we join them into space-separated lines.\n",
        "lines = [\n",
        "    \" \".join(seq) if isinstance(seq, (list, tuple)) else str(seq)\n",
        "    for seq in sequences\n",
        "]\n",
        "\n",
        "OUTPUT_SEQS = \"republic_sequences.txt\"   # goes to .../MyDrive/Colab Notebooks/Predict-Words-Analysis/data/\n",
        "save_doc(lines, OUTPUT_SEQS)\n",
        "\n",
        "print(\"Wrote:\", _resolve_path(OUTPUT_SEQS))"
      ],
      "metadata": {
        "id": "gVwLzPelgTde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b6187cf-4dc8-4f23-c5cb-0d40e13ad05d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote: /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/data/republic_sequences.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify file exists and peek a couple of lines\n",
        "\n",
        "p = _resolve_path(OUTPUT_SEQS)\n",
        "assert p.exists(), f\"Expected file at {p}\"\n",
        "preview = load_doc(OUTPUT_SEQS).splitlines()[:2]\n",
        "print(\"Preview:\", preview)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZNDIZ6awILz",
        "outputId": "a0aea25f-a9c9-45a5-e21b-1617df47bc55"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preview: ['the project gutenberg ebook of the republic by plato this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoeveryou may copy it give it away or re use it under the terms', 'project gutenberg ebook of the republic by plato this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoeveryou may copy it give it away or re use it under the terms of']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement data preparation and tokenization pipeline.\n",
        "\n",
        "# Consolidated Imports\n",
        "\n",
        "import numpy\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential # Use tensorflow namespace\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding # Use tensorflow namespace\n",
        "\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]"
      ],
      "metadata": {
        "id": "p9qVrA81iCdl"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "\n",
        "from pathlib import Path\n",
        "p = Path(in_filename) if in_filename.startswith(\"/content/\") else _resolve_path(in_filename)\n",
        "assert p.exists(), f\"File not found: {p}\"\n"
      ],
      "metadata": {
        "id": "UTtsf2qmxEaZ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell — Train LSTM next-word model and persist artifacts to Google Drive\n",
        "----------------------------------------------------------------------\n",
        "Prereqs:\n",
        "  • You have already computed: X (np.ndarray), y (np.ndarray one-hot),\n",
        "    seq_length (int), vocab_size (int), tokenizer (fitted Keras Tokenizer).\n",
        "  • Drive is mounted:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "Outputs (Drive):\n",
        "  • Model (.keras):  MyDrive/Colab Notebooks/Predict-Words-Analysis/models/nextWordPredict/nextWord.keras\n",
        "  • Tokenizer (.pkl): MyDrive/Colab Notebooks/Predict-Words-Analysis/models/tokenizer.pkl\n",
        "  • Metadata (.json):  MyDrive/Colab Notebooks/Predict-Words-Analysis/models/metadata.json\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import json, pickle, time, platform\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# --------- Preflight checks (fail fast with clear errors) ----------\n",
        "required = {\n",
        "    \"X\": \"numpy.ndarray of shape (n_samples, seq_length)\",\n",
        "    \"y\": \"numpy.ndarray one-hot of shape (n_samples, vocab_size)\",\n",
        "    \"seq_length\": \"int (timesteps used during training)\",\n",
        "    \"vocab_size\": \"int (len(tokenizer.word_index)+1)\",\n",
        "    \"tokenizer\": \"fitted keras.preprocessing.text.Tokenizer\",\n",
        "}\n",
        "for name in required:\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Missing variable `{name}`. Expected: {required[name]}\")\n",
        "if not isinstance(seq_length, int) or seq_length <= 0:\n",
        "    raise ValueError(f\"Bad seq_length: {seq_length}\")\n",
        "if not isinstance(vocab_size, int) or vocab_size <= 1:\n",
        "    raise ValueError(f\"Bad vocab_size: {vocab_size}\")\n",
        "if not hasattr(tokenizer, \"word_index\"):\n",
        "    raise TypeError(\"`tokenizer` doesn’t look like a fitted Keras Tokenizer.\")\n",
        "\n",
        "# Optional additional shape checks\n",
        "assert X.ndim == 2 and X.shape[1] == seq_length, f\"X shape mismatch: {X.shape}, seq_length={seq_length}\"\n",
        "assert y.ndim == 2 and y.shape[1] == vocab_size, f\"y shape mismatch: {y.shape}, vocab_size={vocab_size}\"\n",
        "\n",
        "# --------- Model definition (matches your architecture) ----------\n",
        "embedding_dim = 50\n",
        "lstm_units = 50\n",
        "dense_units = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=seq_length))\n",
        "model.add(LSTM(lstm_units, return_sequences=True))\n",
        "model.add(LSTM(lstm_units))\n",
        "model.add(Dense(dense_units, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Ensure a concrete input shape (optional)\n",
        "model.build(input_shape=(None, seq_length))\n",
        "print(model.summary())\n",
        "\n",
        "# --------- Compile & Train ----------\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "history = model.fit(X, y, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "# --------- Persist artifacts to Google Drive ----------\n",
        "drive_root   = Path(\"/content/drive/MyDrive\")\n",
        "project_root = drive_root / \"Colab Notebooks\" / \"Predict-Words-Analysis\"\n",
        "models_dir   = project_root / \"models\"\n",
        "nw_dir       = models_dir / \"nextWordPredict\"\n",
        "\n",
        "nw_dir.mkdir(parents=True, exist_ok=True)\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_path     = nw_dir / \"nextWord.h5\"\n",
        "tokenizer_path = models_dir / \"tokenizer.pkl\"\n",
        "metadata_path  = models_dir / \"metadata.json\"\n",
        "\n",
        "# Save model\n",
        "model.save(model_path)\n",
        "\n",
        "# --- Save tokenizer (pickle) ---\n",
        "with tokenizer_path.open(\"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "# Save minimal metadata for reproducibility\n",
        "meta = {\n",
        "    \"created_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "    \"seq_length\": int(seq_length),\n",
        "    \"vocab_size\": int(vocab_size),\n",
        "    \"embedding_dim\": int(embedding_dim),\n",
        "    \"lstm_units\": int(lstm_units),\n",
        "    \"dense_units\": int(dense_units),\n",
        "    \"python_version\": platform.python_version(),\n",
        "}\n",
        "with metadata_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# --------- Sanity: existence + quick reload test (lightweight) ----------\n",
        "assert model_path.exists(), f\"Model not saved: {model_path}\"\n",
        "assert tokenizer_path.exists(), f\"Tokenizer not saved: {tokenizer_path}\"\n",
        "assert metadata_path.exists(), f\"Metadata not saved: {metadata_path}\"\n",
        "\n",
        "# Optional: quick load to ensure files aren’t corrupt\n",
        "from tensorflow.keras.models import load_model as _load_model\n",
        "_ = _load_model(model_path, compile=False)  # model reload sanity\n",
        "with tokenizer_path.open(\"rb\") as f:\n",
        "    _tok = pickle.load(f)\n",
        "assert len(getattr(_tok, \"word_index\", {})) == len(tokenizer.word_index), \"Tokenizer mismatch on reload.\"\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\"  Model    :\", model_path)\n",
        "print(\"  Tokenizer:\", tokenizer_path)\n",
        "print(\"  Metadata :\", metadata_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ugzY55D1jAxx",
        "outputId": "c10432a4-1553-4ccb-b320-a62039e0261e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │       \u001b[38;5;34m186,150\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m20,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m2,550\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3723\u001b[0m)           │       \u001b[38;5;34m189,873\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">186,150</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,550</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3723</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">189,873</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m418,973\u001b[0m (1.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">418,973</span> (1.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m418,973\u001b[0m (1.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">418,973</span> (1.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 111ms/step - accuracy: 0.0664 - loss: 7.3267\n",
            "Epoch 2/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 105ms/step - accuracy: 0.0824 - loss: 6.2046\n",
            "Epoch 3/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.0924 - loss: 6.0302\n",
            "Epoch 4/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 104ms/step - accuracy: 0.1190 - loss: 5.9423\n",
            "Epoch 5/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 105ms/step - accuracy: 0.1207 - loss: 5.8113\n",
            "Epoch 6/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 103ms/step - accuracy: 0.1265 - loss: 5.7128\n",
            "Epoch 7/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 103ms/step - accuracy: 0.1312 - loss: 5.6027\n",
            "Epoch 8/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.1346 - loss: 5.4982\n",
            "Epoch 9/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 107ms/step - accuracy: 0.1341 - loss: 5.4289\n",
            "Epoch 10/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 111ms/step - accuracy: 0.1406 - loss: 5.3539\n",
            "Epoch 11/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 107ms/step - accuracy: 0.1462 - loss: 5.2879\n",
            "Epoch 12/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 105ms/step - accuracy: 0.1459 - loss: 5.2516\n",
            "Epoch 13/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 103ms/step - accuracy: 0.1493 - loss: 5.1752\n",
            "Epoch 14/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.1546 - loss: 5.1414\n",
            "Epoch 15/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.1673 - loss: 5.0688\n",
            "Epoch 16/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.1698 - loss: 4.9866\n",
            "Epoch 17/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 103ms/step - accuracy: 0.1687 - loss: 4.9598\n",
            "Epoch 18/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 104ms/step - accuracy: 0.1694 - loss: 4.9447\n",
            "Epoch 19/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 105ms/step - accuracy: 0.1697 - loss: 4.8888\n",
            "Epoch 20/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 106ms/step - accuracy: 0.1714 - loss: 4.8611\n",
            "Epoch 21/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 103ms/step - accuracy: 0.1784 - loss: 4.8336\n",
            "Epoch 22/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.1848 - loss: 4.7589\n",
            "Epoch 23/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.1768 - loss: 4.7511\n",
            "Epoch 24/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 105ms/step - accuracy: 0.1814 - loss: 4.6969\n",
            "Epoch 25/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 103ms/step - accuracy: 0.1831 - loss: 4.6577\n",
            "Epoch 26/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.1857 - loss: 4.6331\n",
            "Epoch 27/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 104ms/step - accuracy: 0.1883 - loss: 4.5784\n",
            "Epoch 28/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 103ms/step - accuracy: 0.1925 - loss: 4.5243\n",
            "Epoch 29/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.1869 - loss: 4.5020\n",
            "Epoch 30/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 109ms/step - accuracy: 0.1978 - loss: 4.4477\n",
            "Epoch 31/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 104ms/step - accuracy: 0.2016 - loss: 4.3860\n",
            "Epoch 32/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 105ms/step - accuracy: 0.2032 - loss: 4.3568\n",
            "Epoch 33/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.1992 - loss: 4.3007\n",
            "Epoch 34/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.1982 - loss: 4.2909\n",
            "Epoch 35/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 104ms/step - accuracy: 0.2004 - loss: 4.2351\n",
            "Epoch 36/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.2053 - loss: 4.2018\n",
            "Epoch 37/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 104ms/step - accuracy: 0.2133 - loss: 4.1329\n",
            "Epoch 38/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 103ms/step - accuracy: 0.2122 - loss: 4.0721\n",
            "Epoch 39/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 103ms/step - accuracy: 0.2138 - loss: 4.0590\n",
            "Epoch 40/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.2228 - loss: 4.0164\n",
            "Epoch 41/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 104ms/step - accuracy: 0.2247 - loss: 3.9466\n",
            "Epoch 42/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 117ms/step - accuracy: 0.2198 - loss: 3.9591\n",
            "Epoch 43/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 110ms/step - accuracy: 0.2281 - loss: 3.8938\n",
            "Epoch 44/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 104ms/step - accuracy: 0.2320 - loss: 3.8502\n",
            "Epoch 45/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 106ms/step - accuracy: 0.2393 - loss: 3.7822\n",
            "Epoch 46/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 108ms/step - accuracy: 0.2383 - loss: 3.7720\n",
            "Epoch 47/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 104ms/step - accuracy: 0.2415 - loss: 3.7466\n",
            "Epoch 48/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.2400 - loss: 3.7361\n",
            "Epoch 49/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 109ms/step - accuracy: 0.2503 - loss: 3.6705\n",
            "Epoch 50/50\n",
            "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 104ms/step - accuracy: 0.2524 - loss: 3.6143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved:\n",
            "  Model    : /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/models/nextWordPredict/nextWord.h5\n",
            "  Tokenizer: /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/models/tokenizer.pkl\n",
            "  Metadata : /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/models/metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Export JSON tokenizer from the notebook\n",
        "\n",
        "from pathlib import Path\n",
        "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
        "\n",
        "drive_root   = Path(\"/content/drive/MyDrive\")\n",
        "project_root = drive_root / \"Colab Notebooks\" / \"Predict-Words-Analysis\"\n",
        "models_dir   = project_root / \"models\"\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tokenizer_json_path = models_dir / \"tokenizer.json\"\n",
        "tokenizer_json_path.write_text(tokenizer.to_json(), encoding=\"utf-8\")\n",
        "\n",
        "# sanity\n",
        "tok2 = tokenizer_from_json(tokenizer_json_path.read_text(encoding=\"utf-8\"))\n",
        "assert len(tok2.word_index) == len(tokenizer.word_index)\n",
        "print(\"Tokenizer JSON saved:\", tokenizer_json_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urLQ6WMLMepX",
        "outputId": "1ea99363-61c2-4b30-a600-6c747b3dff6c"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer JSON saved: /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/models/tokenizer.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement text generation function using a trained Keras model\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "    result = list()\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of words\n",
        "    for _ in range(n_words):\n",
        "        # encode the text as integer\n",
        "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict probabilities for each word\n",
        "        # yhat = model.predict_classes(encoded, verbose=0)\n",
        "        predict_x=model.predict(encoded)\n",
        "        yhat=np.argmax(predict_x,axis=1)\n",
        "        # map predicted word index to word\n",
        "        out_word = ''\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == yhat:\n",
        "                out_word = word\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += ' ' + out_word\n",
        "        result.append(out_word)\n",
        "    return ' '.join(result)"
      ],
      "metadata": {
        "id": "IpAMwTKg6byA"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load text sequences from file and determine sequence length\n",
        "\n",
        "in_filename = 'republic_sequences.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "seq_length = len(lines[0].split()) - 1\n",
        "\n",
        "print(len(lines))\n",
        "print(lines[0])"
      ],
      "metadata": {
        "id": "SrJD7IUbDO0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d4be52d-3149-48bc-fc9c-168ca1053e4b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18332\n",
            "the project gutenberg ebook of the republic by plato this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoeveryou may copy it give it away or re use it under the terms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell — Load next-word model/tokenizer from Google Drive (MyDrive) and generate text\n",
        "-----------------------------------------------------------------------------------\n",
        "Assumes:\n",
        "  • Drive is mounted:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "  • Files are stored under:\n",
        "        /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/data/republic_sequences.txt\n",
        "        /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/models/tokenizer.pkl\n",
        "        /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/models/nextWordPredict/nextWord.h5\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ---- Project-scoped Drive paths ----\n",
        "DRIVE_BASE   = Path(\"/content/drive/MyDrive\")\n",
        "PROJECT_ROOT = DRIVE_BASE / \"Colab Notebooks\" / \"Predict-Words-Analysis\"\n",
        "DATA_DIR     = PROJECT_ROOT / \"data\"\n",
        "MODELS_DIR   = PROJECT_ROOT / \"models\"\n",
        "NEXTWORD_DIR = MODELS_DIR / \"nextWordPredict\"\n",
        "\n",
        "MODEL_PATH     = NEXTWORD_DIR / \"nextWord.h5\"\n",
        "TOKENIZER_PATH = MODELS_DIR   / \"tokenizer.pkl\"\n",
        "SEQS_PATH      = DATA_DIR     / \"republic_sequences.txt\"\n",
        "\n",
        "# ---- Preflight: ensure Drive + files exist ----\n",
        "assert DRIVE_BASE.exists(), (\n",
        "    \"Google Drive not mounted at /content/drive/MyDrive. \"\n",
        "    \"Run: from google.colab import drive; drive.mount('/content/drive')\"\n",
        ")\n",
        "for p in (MODEL_PATH, TOKENIZER_PATH, SEQS_PATH):\n",
        "    assert p.exists(), f\"Missing required file: {p}\"\n",
        "\n",
        "def load_assets(model_path: Path, tokenizer_path: Path):\n",
        "    \"\"\"\n",
        "    Load the trained next-word model and its matching tokenizer.\n",
        "\n",
        "    Args:\n",
        "        model_path: Absolute path to the .keras model file in Drive.\n",
        "        tokenizer_path: Absolute path to the pickled Keras Tokenizer.\n",
        "\n",
        "    Returns:\n",
        "        (model, tokenizer)\n",
        "    \"\"\"\n",
        "    model = load_model(model_path, compile=False)\n",
        "    with tokenizer_path.open(\"rb\") as f:\n",
        "        tokenizer = pickle.load(f)\n",
        "    return model, tokenizer\n",
        "\n",
        "def infer_seq_length(model=None, sequences_path: Path | None = None) -> int:\n",
        "    \"\"\"\n",
        "    Infer the training sequence length.\n",
        "\n",
        "    Priority:\n",
        "      1) model.input_shape[1] if present\n",
        "      2) mode(line_length) - 1 from the sequences file (lines are usually seq_length+1)\n",
        "    \"\"\"\n",
        "    # 1) From model\n",
        "    if model is not None and isinstance(model.input_shape, (list, tuple)):\n",
        "        if len(model.input_shape) >= 2 and isinstance(model.input_shape[1], int):\n",
        "            return int(model.input_shape[1])\n",
        "\n",
        "    # 2) From sequences file\n",
        "    if sequences_path is None:\n",
        "        raise ValueError(\"Need a model with input_shape or a sequences file to infer seq_length.\")\n",
        "    lengths = []\n",
        "    with sequences_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            lengths.append(len(line.split()))\n",
        "            if i > 5000:  # sample is enough to get the mode\n",
        "                break\n",
        "    if not lengths:\n",
        "        raise ValueError(\"Sequences file appears empty.\")\n",
        "    values, counts = np.unique(lengths, return_counts=True)\n",
        "    modal_len = int(values[np.argmax(counts)])\n",
        "    return modal_len - 1\n",
        "\n",
        "def generate_seq_sampling(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    seq_length: int,\n",
        "    seed_text: str,\n",
        "    n_words: int = 20,\n",
        "    *,\n",
        "    temperature: float = 0.9,\n",
        "    top_k: int | None = 50,\n",
        "    top_p: float | None = None,   # e.g. 0.9 (nucleus); use either top_k or top_p\n",
        "    repetition_penalty: float = 1.1,  # >1.0 discourages repeats\n",
        "    recent_window: int = 20\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Sample next words with temperature + top-k/top-p and a light repetition penalty.\n",
        "\n",
        "    Args:\n",
        "        model, tokenizer: your trained pair (tokenizer must match the model).\n",
        "        seq_length: timesteps expected by the model.\n",
        "        seed_text: initial text (will be trimmed/padded to seq_length).\n",
        "        n_words: how many tokens to generate.\n",
        "        temperature: >1.0 = more random, <1.0 = more conservative. Typical 0.7–1.0.\n",
        "        top_k: keep only the k highest-prob tokens before sampling (set None to disable).\n",
        "        top_p: keep smallest set whose cumulative prob ≥ p (nucleus sampling). Use None if using top_k.\n",
        "        repetition_penalty: >1.0 reduces probability of recently used tokens.\n",
        "        recent_window: how many recent tokens to penalize.\n",
        "    \"\"\"\n",
        "    assert not (top_k and top_p), \"Use either top_k or top_p, not both.\"\n",
        "    idx_to_word = getattr(tokenizer, \"index_word\", {})\n",
        "    word_to_idx = tokenizer.word_index\n",
        "\n",
        "    def _sample_id(probs: np.ndarray, recent_ids: list[int]) -> int:\n",
        "        # temperature scaling (operate in log-space to avoid underflow)\n",
        "        logits = np.log(probs + 1e-9) / max(temperature, 1e-6)\n",
        "        probs_t = np.exp(logits)\n",
        "        probs_t /= probs_t.sum()\n",
        "\n",
        "        # repetition penalty on recent ids\n",
        "        if repetition_penalty and recent_ids:\n",
        "            for tid in set(recent_ids[-recent_window:]):\n",
        "                probs_t[tid] /= repetition_penalty\n",
        "            probs_t = np.clip(probs_t, 0, None)\n",
        "            s = probs_t.sum()\n",
        "            if s > 0:\n",
        "                probs_t /= s\n",
        "\n",
        "        # top-k filter\n",
        "        if top_k and top_k > 0:\n",
        "            idxs = np.argpartition(probs_t, -top_k)[-top_k:]\n",
        "            p = probs_t[idxs]\n",
        "            p = p / p.sum()\n",
        "            return int(np.random.choice(idxs, p=p))\n",
        "\n",
        "        # top-p (nucleus) filter\n",
        "        if top_p and 0 < top_p < 1:\n",
        "            sort_idx = np.argsort(-probs_t)\n",
        "            sort_p = probs_t[sort_idx]\n",
        "            cumsum = np.cumsum(sort_p)\n",
        "            cutoff = np.searchsorted(cumsum, top_p, side=\"right\") + 1\n",
        "            idxs = sort_idx[:cutoff]\n",
        "            p = probs_t[idxs]\n",
        "            p = p / p.sum()\n",
        "            return int(np.random.choice(idxs, p=p))\n",
        "\n",
        "        # plain multinomial sampling\n",
        "        return int(np.random.choice(len(probs_t), p=probs_t))\n",
        "\n",
        "    in_text = seed_text.strip()\n",
        "    recent_ids: list[int] = []\n",
        "    for _ in range(n_words):\n",
        "        enc = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        enc = pad_sequences([enc], maxlen=seq_length, truncating=\"pre\")\n",
        "        probs = model.predict(enc, verbose=0)[0]  # softmax over vocab\n",
        "        next_id = _sample_id(probs, recent_ids)\n",
        "        next_word = idx_to_word.get(next_id)\n",
        "        if not next_word:\n",
        "            break\n",
        "        in_text += \" \" + next_word\n",
        "        recent_ids.append(next_id)\n",
        "    return in_text\n",
        "\n",
        "def pick_seed_from_sequences(seqs_path: Path, seq_length: int) -> str:\n",
        "    \"\"\"\n",
        "    Pick a seed from the training sequences file and trim to seq_length tokens.\n",
        "    \"\"\"\n",
        "    with seqs_path.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        lines = [l.strip() for l in f if l.strip()]\n",
        "    idx = np.random.randint(len(lines))\n",
        "    seed_line = lines[idx]\n",
        "    return \" \".join(seed_line.split()[:seq_length])\n",
        "\n",
        "# ---- Load assets ----\n",
        "model, tokenizer = load_assets(MODEL_PATH, TOKENIZER_PATH)\n",
        "\n",
        "# ---- Derive sequence length ----\n",
        "try:\n",
        "    seq_length = infer_seq_length(model=model)\n",
        "except Exception:\n",
        "    seq_length = infer_seq_length(model=None, sequences_path=SEQS_PATH)\n",
        "\n",
        "# ---- Sanity: tokenizer vocab should not exceed embedding input_dim (if present) ----\n",
        "emb_input_dim = next((getattr(l, \"input_dim\", None) for l in model.layers if hasattr(l, \"input_dim\")), None)\n",
        "vocab_size = len(getattr(tokenizer, \"word_index\", {})) + 1\n",
        "assert emb_input_dim is None or vocab_size <= emb_input_dim, (\n",
        "    f\"Tokenizer vocab ({vocab_size}) exceeds model embedding input_dim ({emb_input_dim}). \"\n",
        "    \"Likely a mismatched tokenizer/model pair.\"\n",
        ")\n",
        "\n",
        "# ---- Pick a seed and generate ----\n",
        "seed_text = pick_seed_from_sequences(SEQS_PATH, seq_length)\n",
        "print(\"SEED:\", seed_text, \"\\n\")\n",
        "\n",
        "generated = generate_seq_sampling(\n",
        "    model, tokenizer, seq_length, seed_text,\n",
        "    n_words=30, temperature=0.9, top_k=50, repetition_penalty=1.15\n",
        ")\n",
        "print(generated)\n",
        "\n"
      ],
      "metadata": {
        "id": "bk-uudFsGjLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c762ae55-b1c8-4bcd-a08a-a359804305c3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SEED: and then again shepherd or ruler in an inexact if the words are strictly taken the ruler and the shepherd look only to the good of their people or flocks and not to their own whereas you insist that rulers are solely actuated by love of officeno doubt about it \n",
            "\n",
            "and then again shepherd or ruler in an inexact if the words are strictly taken the ruler and the shepherd look only to the good of their people or flocks and not to their own whereas you insist that rulers are solely actuated by love of officeno doubt about it or induced of antiquity by such again could if he testifies allnowhere in the state that the wicked comprehends be father and as them is is his inability are all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Cell — Export artifacts for Django (macOS Intel / TF 2.10 compatible)\n",
        "---------------------------------------------------------------------\n",
        "Writes:\n",
        "  • H5 model     → .../models/nextWordPredict/nextWord.h5\n",
        "  • Tokenizer    → .../models/tokenizer.pkl\n",
        "  • Metadata     → .../models/metadata.json\n",
        "Also does a quiet reload sanity check with compile=False.\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "import json, pickle, time, platform\n",
        "from tensorflow.keras.models import load_model as _load_model\n",
        "\n",
        "# --- Pre-reqs in memory: model, tokenizer, seq_length, vocab_size ---\n",
        "assert \"model\" in globals(), \"model not found\"\n",
        "assert \"tokenizer\" in globals(), \"tokenizer not found\"\n",
        "assert \"seq_length\" in globals() and isinstance(seq_length, int)\n",
        "assert \"vocab_size\" in globals() and isinstance(vocab_size, int)\n",
        "\n",
        "# --- Drive project paths (adjust if yours differ) ---\n",
        "drive_root   = Path(\"/content/drive/MyDrive\")\n",
        "project_root = drive_root / \"Colab Notebooks\" / \"Predict-Words-Analysis\"\n",
        "models_dir   = project_root / \"models\"\n",
        "nw_dir       = models_dir / \"nextWordPredict\"\n",
        "nw_dir.mkdir(parents=True, exist_ok=True)\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "h5_path       = nw_dir / \"nextWord.h5\"         # Django/TF 2.10 will load this\n",
        "tokenizer_path= models_dir / \"tokenizer.pkl\"\n",
        "metadata_path = models_dir / \"metadata.json\"\n",
        "\n",
        "# --- Save artifacts ---\n",
        "model.save(h5_path)  # HDF5 (Keras 2.x friendly)\n",
        "with tokenizer_path.open(\"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "\n",
        "meta = {\n",
        "    \"created_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "    \"seq_length\": int(seq_length),\n",
        "    \"vocab_size\": int(vocab_size),\n",
        "    \"export_format\": \"h5\",\n",
        "    \"python_version\": platform.python_version(),\n",
        "}\n",
        "with metadata_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# --- Quiet reload sanity (compile=False avoids warnings; not needed in Django) ---\n",
        "_ = _load_model(h5_path, compile=False)\n",
        "with tokenizer_path.open(\"rb\") as f:\n",
        "    _tok = pickle.load(f)\n",
        "assert len(_tok.word_index) == len(tokenizer.word_index)\n",
        "\n",
        "print(\"Exported for Django:\")\n",
        "print(\"  H5 model  :\", h5_path)\n",
        "print(\"  Tokenizer :\", tokenizer_path)\n",
        "print(\"  Metadata  :\", metadata_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEMRVG63y7rm",
        "outputId": "9d8106eb-be38-4dd7-c7f9-1fbf1bf14eb2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported for Django:\n",
            "  H5 model  : /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/models/nextWordPredict/nextWord.h5\n",
            "  Tokenizer : /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/models/tokenizer.pkl\n",
            "  Metadata  : /content/drive/MyDrive/Colab Notebooks/Predict-Words-Analysis/models/metadata.json\n"
          ]
        }
      ]
    }
  ]
}